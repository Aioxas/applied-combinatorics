% kitchen-sink.tex
% Updated January 11, 2012

\chapter{The Many Faces of Combinatorics}\label{ch:kitchensink}

\section{On-line algorithms}

Many applications of combinatorics occur in a dynamic, on-line
manner.  It is rare that one has all the information about the
challenges a problem presents before circumstances compel that
decisions be made.  As examples, a decision to proceed with a
major construction construction project must be made several
years before ground is broken; investment decisions are made
on the basis of today's information and may look particularly
unwise when tomorrow's news is available; and deciding to
exit a plane with a parachute is rarely reversible.

In this section, we present two examples intended to illustrate
on-line problems in a combinatorial setting.  Our first example
involves graph coloring.  As is customary in discussions of 
on-line algorithms, we consider a two-person game with the players
called \textit{Assigner} and \textit{Builder}.  The two players
agree in advance on a class $\cgC$ of graphs, and the game
is played in a series of rounds.  At round~$1$ Builder presents
a single vertex, and Assigner assigns it a color.  At each
subsequent rounds, Builder presents a new vertex, and provides 
complete information at to which of the preceding vertices are 
adjacent to it.  In turn, Assigner must give the new vertex a 
color distinct from colors she has assigned previously to its 
neighbors.

\begin{example}\label{exa:P4}
Even if Builder is constrained to build a path on $4$ vertices,
then Assigner can be forced to use three colors.
At Round~1, Builder presents a vertex $x$ and Assigner colors 
it.  At Round~2, Builder presents a vertex $y$ and declares that 
$x$ and $y$ are not adjacent.

Now Assigner has a choice.  She may either give $x$ and $y$ the
same color, or she may elect to assign a new color to $y$.
If Assigner gives $x$ and $y$ different colors, then in Round~3, 
Builder presents a vertex $z$ and declares that $z$ is adjacent to 
both $x$ and $y$.  Now  Assigner will be forced to use a third 
color on $z$.  In Round~$4$, Builder will add a vertex $w$ adjacent
to $y$ but to neither $x$ nor $z$, but the damage has already been
done.

On the other hand, if Assigner $x$ and $y$ the same color, then 
in Round~3, Builder presents a vertex $z$, with $z$ adjacent to 
$x$ but not to $y$.  Assigner must use a second color on $z$, 
distinct from the one she gave to $x$ and $y$.  In Round~4, Builder 
presents a vertex $w$ adjcent to $z$ and $y$ but not to $x$.  Assigner 
must use a third color on $w$.  
\end{example}

Note that a path is a tree and trees are forests.  The next result
shows that while forests are trivial to color off-line, there is
a genuine challenge ahead when you have to work on-line.  To assist
us in keeping track of the colors used by Assigner, we will
use the notation from \autoref{ch:graphs} and write $\phi(x)$ for
the color given by Assigner to vertex $x$.

\begin{theorem}\label{thm:olforest}
Let $n$ be a positive integer.  Then there is a strategy for
Builder that will enable Builder to construct a forest having
at most $2^{n-1}$ vertices while forcing
Assigner to use $n$ colors.
\end{theorem}

\begin{proof}
When $n=1$, all Builder does is present a single vertex.  When
$n=2$, two adjacent vertices are enough.  When $n=3$, Builder
constructs a path on $4$ vertices as detailed in Example~\ref{exa:P4}.
Now assume that for some $k\ge3$, Builder has a strategy $S_i$ for
forcing Assigner to use $i$ colors on a forest of at most $2^{i-1}$
vertices, for each $i=1,2,\dots,k$.  Here's how Builder proceeds
to force $k+1$ colors.

First, for each $i=1,2,\dots,k$, Builder follows strategy $S_i$
to build a forest $F_i$ having at most $2^{i-1}$ vertices on which
assigner is forced to use $i$ colors.  Furthermore, when $1\le i<j\le k$,
there are no edges between vertices in $F_i$ and vertices in $F_j$.

Next, Builder chooses a vertex $y_1$ from $F_1$.  Since 
Assigner uses two colors on $F_2$, there is a vertex $y_2$ from
$F_2$ so that $\phi(y_2)\neq \phi(y_1)$.  Since Assigner uses
three colors on $F_3$, there is a vertex $y_3$ in $F_3$ so that
$\{\phi(y_1),\phi(y_2),\phi(y_3)\}$ are all distinct.  It follows
that Builder may identify vertices $y_1$, $y_2,\dots,y_k$ with
$y_i\in F_i$ so that the colors $\{\phi(y_i:1\le i\le k\}$ are
all distinct.  Builder now presents a new vertex $x$ and 
declares $x$ adjacent to all vertices in $\{y_1,y_2,\dots,y_k\}$ and to
no other vertices.  Clearly, (1)~the resulting graph is a forest; 
(2)~Assigner is forced to use a color for $x$ distinct from
the $k$ colors she assigned previously to the vertices in $\{y_1,y_2,\dots,
y_k\}$.  Also, the total number of vertices is at most $1+[1+2+4+8+\dots+2^{k-1}]=2^k$.
\end{proof}

Bob reads the proof and asks whether it was really necessary to treat the 
cases $k=2$ and $k=3$ separately.  Wasn't it enough just to note that the
case $k=1$ holds trivially.  Carlos says yes.

\subsection{Doing Relatively Well in an On-Line Setting}

Theorem~\ref{thm:olforest} should be viewed as a negative
result.  It is hard to imagine a family of graphs easier to color
than forests, yet in an on-line setting, graphs in this family are 
difficult to color.  On the other hand, in certain settings, one can 
do reasonably well in an on-line setting, perhaps not as well as 
the true optimal off-line result but good enough
to be useful.  Here we present a particularly elegant example
involving partially ordered sets.

Recall that a poset $P$ of height $h$ can be partitioned into $h$
antichains---by recursively removing the set of minimal elements.
But how many antichains are required in an on-line setting?
Now Builder constructs a poset $P$ one point at a time,
while Assigner constructs a partition of $P$ into antichains.
At each round, Builder will present a new points $x$, and list
those points presented earlier that are, respectively, less than $x$,
greater than $x$ and incomparable with $x$.  Subsequently,
Assigner will assign $x$ to an antichain.  This will be done
either by adding $x$ to an antichain already containing one
or more of the points presented previously, or by assigning $x$
to a new antichain.

\begin{theorem}
For each $h\ge1$, there is a on-line strategy for Assigner that will enable
her to partition a poset $P$ into at most $\binom{h+1}{2}$
antichains, provided the height of $P$ is at most $h$.
\end{theorem}
\begin{proof}
It is important to note that Assigner does not need to know the value
$h$ in advance.  For example, Builder may have in mind that ultimately
the value of $h$ will be $300$, but this information does not
impact Assigner's strategy.

When the new point $x_n$ enters $P$, Assigner computes the values
$r$ and $s$, where $r$ is the largest integer for which there
exists a chain $C$ of $r$ points in $\{x_1,x_2,\dots,x_n\}$ having
$x_n$ as its least element.  Also, $s$ is the largest integer for which there
exists a chain $D$ of $s$ points in $\{x_1,x_2,\dots,x_n\}$ having
$x_n$ as its largest element.  Assigner then places $x$ in a set
$A(r,s)$, claiming that any two points in this set are incomparable.
To see that this claim is valid, consider the first moment where
Builder has presented a new point $x$, Assigner places $x$ in 
$A(r,s)$ and there is already a point $y$ in $A(r,s)$ for which
$x$ and $y$ are comparable.

When $y$ was presented, there was at that moment in time a chain $C'$ of $r$ 
points having $y$ as its least element.  Also, there was a chain
$D$ of $s$ points having $y$ as its greatest element.  

Now suppose that $y>x$ in $P$.  Then we can add $x$ to $C'$ to
form a chain of $r+1$ points having $x$ as its least element.
This would imply that $x$ is not assigned in $A(r,s)$.  Similarly,
if $y<x$ in $P$,  then we may add $x$ to $D'$ to form a chain
of $s+1$ points having $x$ as its greatest element.  Again, this
would imply that $x$ is not assigned to $A(r,s)$.

So Assigner has indeed devised a good strategy for partitioning
$P$ into antichains, but how many antichains has she used?  
This is just asking how many ordered pairs $(i,j)$ of positive integers
are there subject to the restriction that $i+j-1\le h$.  And we
learned how to solve this kind of question in \autoref{ch:strings}.
The answer of course is $\binom{h+1}{2}$.
\end{proof}

The strategy for Assigner is so simple and natural, it might be
the case that a more complex strategy would yield a more efficient
partitioning.   Not so.

\begin{theorem} [Szemer\'edi]
For every $h\ge1$, there is a strategy $S_h$ for builder
that will enable him to build a poset $P$ of height $h$ so
that assigner is forced to (1)~use at least $\binom{h+1}{2}$
antichains in partitioning $P$, and (2)~use at least $h$
different antichains on the set of maximal elements.
\end{theorem}

\begin{proof}
Strategy $S_1$ is just to present a single point.
Now suppose that the theorem holds for some integer $h\ge1$.
We show how strategy $S_{h+1}$ proceeds.

First Builder follows strategy $S_h$ to form a poset $P_1$.
Then he follows it a second time for form a poset $P_2$, with
all points of $P_1$ incomparable to all points in $P_2$.
Now we consider two cases.  Suppose first that Assigner
has used $h+1$ or more antichains on the set of maximal elements of
$P_1\cup P_2$.  In this case, he follows strategy $S_h$
a third time to build a poset $P_3$ with all points of
$P_3$ less than all maximal elements of $P_1\cup P_2$
and incomparable with all other points.

Clearly, the height of the resulting poset is at most $h+1$.
Also, Assigner must use $h+1+\binom{h+1}{2}=\binom{h+2}{2}$
antichains in partitioning the poset and she has used $h+1$ on
the set of maximal elements.

So it remains only to consider the case where Assigner has used
a set $W$ of $h$ antichains on the maximal elements of $P_1$, and
she has used exactly the same $h$ antichains for the maximal elements
of $P_2$.  Then Builder presents a new point $x$
and declares it to be greater than all points of $P_1$ and
incomparable with all points of $P_2$.  Assigner must put $x$
in some antichain which is not in $W$.

Builder then follows strategy $S_h$ a third time, but
now all points of $P_3$ are less than $x$ and the maximal elements
of $P_2$.   Again, Assigner has been forced to use $h+1$ different
antichains on the maximal elements and $\binom{h+2}{2}$
antichains altogether.
\end{proof}

\section{Extremal Set Theory}

Let $n$ be a positive integer and let $[n]=1,2,\dots,n$.  In this
section, we consider problems having the following general form:
What is the maximum size of a family of subsets of $[n]$ when the
family is required to satisfy certain properties.

Here is an elementary example.

\begin{example}\label{exa:intersecting}
The maximum size of a family $\cgF$ of subsets of $[n]$, with
$A\cap B\neq\emptyset$ for all $A,B\in\cgF$, is $2^{n-1}$.

For the lower bound, consider the family $\cgF$ of all subsets of
$[n]$ that contain~$1$.  Clearly this family has $2^{n-1}$ elements
and any two sets in the family have non-empty intersection.

For the upper bound, let $\cgF$ be a family of subsets with
each pair of sets in $\cgF$ having non-empty intersection.
Then whenever a subset $S$ is a member of $\cgF$,  the complement $S'$ of 
$S$ cannot belong to $\cgF$. Since the entire family of
all $2^n$ subsets of $[n]$ can be considered as $2^{n-1}$
complementary pairs, and at most one set from each pair
can belong to $\cgF$, we conclude that $|\cgF|\le 2^{n-1}$.
\end{example}

As a second example, we can revisit
Sperner's Theorem from the chapter on partially ordered sets and
restate the result as follows.

\begin{example}\label{exa:Sperner}  
The maximum size of a family $\cgF$ of
subsets of $[n]$ subject to the constraint that when $A$ and $B$
are distinct sets in $\cgF$, then neither is a subset of the other,
is $\binom{n}{\lfloor n/2\rfloor}$.
\end{example}

It is worth noting that in Example~\ref{exa:Sperner},
there is a very small number (one or two) extremal families, i.e.,
when $\cgF$ is a family of subsets of $[n]$, $|\cgF|=
\binom{n}{\lfloor n/2\rfloor}$, and no set in $\cgF$ is
a proper subset of another, then either $\cgF=\{S\subseteq[n]:
|S|=\lfloor n/2\rfloor\}$ or $\cgF=\{S\subseteq[n]:
|S|=\lceil n/2\rceil\}$.  And of course, when $n$ is even, these
are exactly the same family.

On the other hand, for Example~\ref{exa:intersecting}, there
are many extremal families, since for every complementary
pair of sets, either member can be selected.

We close this brief tasting of extremal set theory with a real
classic.

\begin{theorem}[Erd\"os, Ko, Rado]
Let $n$ and $k$ be positive integers with $n\ge 2k$.  Then
the maximum size of a family $\cgF$ of subsets of $[n]$ subject to the
restrictions that (1)~$A\cap B\neq\emptyset$ for all $A,B\in\cgF$,
and (2)~$|A|=k$ for all $A\in\cgF$, is $\binom{n-1}{k-1}$.
\end{theorem}
\begin{proof}
For the lower bound, consider the family $\cgF$ of all $k$ element
subset of $[n]$ that contain~$1$. 

For the upper bound, let $\cgF$ be a family of subsets of $[n]$ 
satisfying the two constraints.  We show that $|\cgF|\le\binom{n-1}{k-1}$.
To accomplish this, we consider a circle in the Euclidean
plane with $n$ points $p_1$, $p_2,\dots,p_n$ equally
spaced points around its circumference.  Then there are
$n!$ different ways (one for each permutation $\sigma$ of $[n]$) 
to place the integers in $[n]$ at the points in $\{p_1,p_2,\dots,p_n\}$
in one to one manner.

For each permutation $\sigma$ of $[n]$, let $\cgF(\sigma)$ denote
the subfamily of $\cgF$ consisting of all sets $S$ from $\cgF$ whose
elements occur in a consecutive block around the circle.  Then
let $t=\sum_\sigma|\cgF(\sigma)|$.

\medskip
\noindent
\textbf{Claim 1.}\quad
$t\le kn!$.

\medskip
\noindent
\textit{Proof.}\quad
Let $\sigma$ be a permutation and suppose that
$|\cgF(\sigma)|=s \ge 1$. Then the union of the
sets from $\cgF(\sigma)$ is a set of points that form a
consecutive block of points on the circle.  Note that
since $n\ge 2k$, this block does not encompass the entire
circle.  Accordingly there is a set $S$ whose elements
are the first $k$ in a clockwise sense within this block.
Since each other set in $\cgF$ represents a clockwise shift
of one of more positions, it follows immediately that 
$|\cgF|\le k$.  Since there are $n!$ permutations, the
claim follows.

\medskip
\noindent
\textbf{Claim 2.}\quad
For each set $S\in\cgF$, there are exactly
$n k!(n-k)!$ permutations $\sigma$ for which $S\in\cgF(\sigma)$.

\medskip
\noindent
\textit{Proof.}\quad
There are $n$ positions around the circle and each can be used
as the first point in a block of $k$ consecutive positions in
which the elements of $S$ can be placed.  Then there are $k!$
ways to order the elements of $S$ and $(n-k)!$ ways to order
the remaining elements.  This completes the proof of the claim.

\medskip
To complete the proof of the theorem, we note that we have
\[
|\cgF| n k! (n-k)!\le t\le k n!
\]
And this implies that $|\cgF|le\binom{n-1}{k-1}$.
\end{proof}

\section{Markov Chains}

We begin this section with a motivational example.
Consider the connected graph on six
vertices shown in \autoref{fig:markovchain}.
The first move is to choose a vertex at random and move
there.  Afterwards, we follow the following recursive
procedures.  If after $i$ moves, 
you are at a vertex $x$
and $x$ has $d$ neighbors, choose one of the neighbors
at random, with each having probability $1/d$ and move there.
We then attempt to answer questions of the following
flavor:

\begin{enumerate}
\item For each vertex $x$, let $p_{x,m}$ denote
the probability that you are at vertex $x$ after $m$
moves.  Does $\lim_{m\rightarrow\infty}p_{x,m}$ exist and
if so, how fast does the sequence converge to this limit?
\item  How many moves must I make in order that the probability
that I have walked on every edge in the graph is at least
$0.999$?
\end{enumerate}

This example illustrates the characteristics of an important class
of computational and combinatorial problems, which are collectively
referred to as \textit{Markov Chains}:  
\begin{enumerate}
\item There is a finite set of states $S_1$, $S_2,\dots,S_n$, and at
time $i$, you are in one of these states.
\item If you are in state $S_j$ at time $i$, then for each
$k=1,2,\dots,n$, there is a fixed probabilty $p(j,k)$ (which does
not depend on $i$) that you will be in state $S_k$ at time $i+1$.
\end{enumerate}

The $n\times n$ matrix $P$ whose $j,k$ entry is the probability $p(j,k)$ 
of moving from state $S_j$ to state $S_k$ is called the \textit{transition}
matrix of the Markov chain.  Note that $P$ is a \textit{stochastic}
matrix, i.e., all entries are non-negative and all row sums are~$1$.
Conversely, each square stochastic matrix can be considered as the transition 
matrix of a Markov chain.

For example, here is the transition matrix for the graph shown in
\autoref{fig:markovchain}.
\begin{equation}\label{eqn:transition}
P =\begin{pmatrix}
      0 & 1/4 & 1/4 & 1/4 & 1/4 &   0 \\
    1/2 &   0 &   0 & 1/2 &   0 &   0 \\
    1/3 &   0 &   0 & 1/3 &   0 & 1/3 \\
    1/3 & 1/3 & 1/3 &   0 &   0 &   0 \\
      1 &   0 &   0 &   0 &   0 &   0 \\
      0 &   0 &   1 &   0 &   0 &   0 \\
   \end{pmatrix}
\end{equation} 
A transition matrix $P$ is \textit{regular} if there is some integer $m$
for which the matrix $P^m$ has only positive entries.  Here is a
fundamental result from this subject, one that is easy to understand
but a bit too complex to prove given our space constraints.

\begin{theorem}\label{thm:regular}
Let $P$ be a regular $n\times n$ transition matrix.  Then 
there is a row vector $W=(w_1, w_2,\dots,w_n$) of positive real
numbers summing to $1$ so that as $m$ tends to infinity, each 
row of $P^m$ tends to $W$.  Furthermore, $WP=W$, and for each
$i=1,2,\dots,n$, the value $w_i$ is the limiting probability of 
being in state $S_i$.
\end{theorem}

Given the statement of \autoref{thm:regular}, the computation of
the row vector $W$ can be carried out by eigenvalue
techniques that are part of a standard undergraduate linear
algebra course.  For example, the transition matrix $P$ displayed in 
Equation~\ref{eqn:transition} is regular since all entries of 
$P^3$ are positive.  Furthermore, for this matrix, the row vector 
$W=(5/13, 3/13, 2/13, 2/13, 1/13, 1/13)$.  However, the question involving how fast
the convergence of $P^m$ is to this limiting vector is more
subtle, as is the question as to how long it takes for us to
be relatively certain we have made every possible transition.

\subsection{Absorbing Markov Chains}
 
A state $S_i$ in a Markov chain with transition matrix $P$ is \textit{absorbing} 
if $p_{i,i}=1$ and $p_{i,j}=0$ for all $j\neq i$, i.e., like the infamous
Hotel California, once you are in state $S_i$, ``you can never leave.''

\begin{example}
We modify the transition matrix from Equation~\ref{eqn:transition} by 
making states $4$ and $5$ absorbing.  The revised transition matrix is now:

\begin{equation}\label{eqn:absorbing}
P =\begin{pmatrix}
      0 & 1/4 & 1/4 & 1/4 & 1/4 &   0 \\
    1/2 &   0 &   0 & 1/2 &   0 &   0 \\
    1/3 &   0 &   0 & 1/3 &   0 & 1/3 \\
    1/3 & 1/3 & 1/3 &   0 &   0 &   0 \\
      0 &   0 &   0 &   0 &   1 &   0 \\
      0 &   0 &   0 &   0 &   0 &   1 \\
   \end{pmatrix}
\end{equation} 
\end{example}
Now we might consider the following game.  Start at one of the
four vertices in $\{1,2,3,4\}$ and proceed as before, making
moves by choosing a neighbor at random.  Vertex $4$ might
be considered as an ``escape'' point, a safe harbor that once
reached is never left.  On the other hand, vertex $5$ might
be somewhere one meets a hungry tiger and be absorbed in a
way not to be detailed here.

We say the Markov chain is \textit{absorbing} if there is at least one
absorbing state and for each state $S_j$ that is not absorbing, it is possible
to reach an absorbing state---although it may take many steps to do so.
Now the kinds of questions we would like to answer are:

\begin{enumerate}
\item If we start in non-absorbing state $S_i$, what is the probability
of reaching absorbing state $S_j$ (and then being absorbed in that state,
a question which takes on genuine unpleasantness relative to tigers)?
\item If we are absorbed in state $S_j$, what is the probability
that we started in non-absorbing state $S_i$? 
\item If we start in non-absorbing state $S_i$, what is the expected length of
time before we will be absorbed?
\end{enumerate}

\section{Miscellaneous Gems}

\subsection{The Stable Matching Theorem}

First, a light hearted optimization problem with a quite
clever solution, called the \textit{Stable Matching Theorem}.  
There are $n$ eligble males $b_1$, $b_2,\dots,b_n$ and
$n$ eligible females $g_1$, $g_2,\dots,g_n$.  We will arrange $n$
marriages, each involving one male and one female. In the process,
we will try to make everyone happy---or at least we will try to
keep things stable.

Each female linearly orders the males in the order of her preference, i.e.,
for each $i=1,2,\dots,n$, there is a permutation $\sigma_i$ of
$[n]$ so that if $g_i$ prefers $b_j$ to $b_k$, then $\sigma_i(j)>
\sigma_i(k)$.  Different females may have quite different
preference orders.  Also, each male linearly orders the females in order of   
his preference, i.e., for each $i=1,2,\dots,n$, there is a permutation
$\tau_i$ of $[n]$ so that if $b_i$ prefers $g_j$ to $g_k$, then
$\tau_i(j)>\tau(k)$.

A $1$--$1$ matching of the $n$ males to the $n$ females is \textit{stable}
if there does not exist two males $b$ and $b'$ and two females
$g$ and $g'$ so that (1)~$b$ is matched to $g$; (2)~$b$ prefers $g'$ to
$g$; and (3)~$g$ prefers $b'$ to $b$.  The idea is that given these
preferences, $b$ and $g$ may be mutually inclined to dissolve their relationship
and initiate daliances with other partners.

So the question is whether, regardless of their respective preferences, we can always
generate a stable matching.  The answer is yes and there is a quite
clever argument, one that yields a quite efficient algorithm.
At Stage~1, all males go knock on the front door of the female which
is tops on their list.  It may happen that some females have more than
one caller while others have none.  However, if a female has
one or more males at her door, she reaches out and grabs the one
among the group which she prefers most by the collar and tells the others, if there
are any, to go away.  Any male rejected at this step proceeds to the
front door of the female who is second on their list.  Again, a female
with one or more suitors at her door chooses the best among then
and sends the others away.  This process continues until eventually,
each female is holding onto exactly one male.

It is interesting to note that each female's prospects improve over time, i.e.,
once she has a suitor, things only get better.  Conversely, each male's
prospects deteriorate over time.  Regardless, we assert that the resulting
matching is stable.  To see this, suppose that it is unstable and
choose males $b$ and $b'$, females $g$ and $g'$ so that $b$ is matched
to $g$, but $b$ prefers $g'$ to $g$ while $g$ prefers $b'$ to $b$.  The
algorithm requires that male $b$ start at the top of his list and
work his way down.  Since he eventually lands on $g$'s door step, and
he prefers $g'$ to $g$, it implies that once upon a time, he was
actually at $g'$'s door, and she sent him away.  This means that at that
exact moment, she had a male in hand that she prefers to $b$.  Since
her holdings only improve with time, it means that when the matching
is finalized, female $g$ has a mate $b$ that she prefers to $b'$.

\section{Zero--One Matrices}

Matrices with all entries $0$ and $1$ arise in many combinatorial
settings, and here we present a classic result, called the Gale/Ryzer
theorem.  It deals with zero--one matrices with specified row and
column sum strings.  When $M$ is an $m\times n$ zero--one matrix, the
string $R=(r_1,r_2,\dots,r_m)$, where $r_i=\sum_{1\le j\le n}m_{i,j}$,
is called the \textit{row sum string} of $M$. The \textit{column sum string}
$C=(c_1,c_2,\dots,c_n)$ is defined analogously.  Conversely,  let $m$ 
and $n$ be positive integers, and let $R=(r_1,r_2,\dots,r_m)$ and 
$C=(c_1,c_2,\dots,c_n)$ be strings of non-negative integers.  The 
question is whether there exists an $m\times n$ zero--one matrix 
$M$ with row sum string $R$ and column sum string $C$.

To attack this problem, we pause briefly to develop some additional
background material.  Note that we may assume without loss of
generality that there is a positive integer $t$ so that 
$\sum_{i=1}^mr_i=\sum_{j=1}^nc_j=t$, else there is certainly
no zero--one matrix with row sum string $R$ and column sum string $C$.
Furthermore, we may assume that both $R$ and $C$ are non-increasing strings, 
i.e., $r_1\ge r_2\ge \dots\ge r_m$ and $c_1\ge c_2\ge\dots\ge c_n$.  

To see this note that whenever we exchange two rows in a zero--one matrix,
the column sum string is unchanged.   Accordingly after a suitable
permutation of the rows, we may assume that $R$ is non-increasing.
Then the process is repeated for the columns.

Finally, it is easy to see that we may assume that all entries in 
$R$ and $C$ are positive integers, since zeroes in these strings
correspond to rows of zeroes or columns of zeroes in the matrix.
Accordingly, the row sum string $R$ and the colum sum string $C$ can be
viewed as partitions of the integer $t$, a topic we first introduced
in \autoref{ch:genfunction}.  

For the balance of this section, we let $t$ be a positive integer
and we let $\cgP(t)$ denote the family of all partitions of the integer
$t$.  There is a natural 
partial order on $\cgP(t)$ defined by setting $V=(v_1,v_2,\dots,v_m)
\ge W=(w_1,w_2,\dots,w_n)$ if and only if $m\le n$ and $\sum_{1\le i\le j}v_j\ge
\sum_{1\le i\le j}w_j$ for each $j=1,2,\dots,m$, i.e., the sequence 
of partial sums for $V$ is always at least as large, term by term, 
as the sequence of partial sums of $W$.  For example, we show in 
\autoref{fig:partitionlattice} the partial order $\cgP(7)$.

FIGURE HERE

In the proof of the Gale-Ryser theorem, it will be essential
to fully understand when one partition covers another.  We state
the following proposition for emphasis; the proof consists
of just considering the details of the definition of the partial
order on partitions.

\begin{proposition}]\label{prop:partitioncover}
Let $V=(v_1,v_2,\dots,v_m)$ and $W=(w_1,w_2,\dots,w_n)$
be partitions of an integer~$t$, and suppose that $V$ covers $W$ in
the poset $\cgP(t)$.  Then $n\le m+1$, and there exist integers $i$ and $j$ with
$1\le i<j\le n$ so that:
\begin{enumerate}
\item $v_\alpha=w_\alpha$, when $1\le \alpha<i$.
\item $v_\beta=w_\beta$, when $j<\beta\le m$.
\item $v_i=1+w_i$.
\item Either (a)~$j\le m$ and $w_j=1+v_j$, or (b)~$j=n=m+1$ and $w_j=1$.
\item If $j>i+1$, then $w_\gamma=v_\gamma=v_i-1$ when $i<\gamma<j$.
\end{enumerate}
\end{proposition}

To illustrate this concept, note that $(6,6,4,3,3,3,1,1,1,1,)$ covers
$(6,6,3,3,3,3,2,1,1,1)$ in $\cgP(29)$.  Also, $(5,4,3)$ covers $(5,3,3,1)$ in
$\cgP(12)$.

With a partition $V=(v_1,v_2,\dots,v_m)$ from $\cgP(t)$,
we associate a \textit{dual partition} $W=(w_1,w_2,\dots,w_n)$ defined
as follows:  (1)~$n=v_1$ and for each $j=1,\dots,n$, $w_j$ is the
number of entries in $V$ that are at least $n+1-j$.  For example, the dual 
partition of $V=(8,6,6,6,5,5,3,1,1,1)$ is $(8,7,7,6,6,4,1,1)$.
Of course, they are both partitions of $42$, which is the secret
of the universe!  In what follows, we denote the dual of the partition 
$V$ by $V^d$.  Note that if $W=V^d$, then $V=W^d$, i.e., the dual of the 
dual is the original.

\subsection{The Obvious Necessary Condition}

Now let $M$ be a $m\times n$ zero--one matrix with row sum string
$R=(r_1,r_2,\dots,r_m$ and column sum string $C=(c_1,c_2,\dots,c_n)$.
As noted before, we will assume that all entries in $R$ and $C$ are positive.
Next, we modify $M$ to form a new matrix $M'$ as follows:  For each $i=1,2,\dots,t$,
we push the $r_i$ ones in row~$i$ as far to the left as possible, i.e., 
$m'_{i,j}=1$ if and only if $1\le j\le r_i$.  Note that $M$ and $M'$
both have $R$ for their row sum strings.  However, if $C'$ denotes
the column sum string for $M'$, then $C'$ is a non-decreasing string,
and the substring $C''$ of $C'$ consisting of the positive entries
is $R^d$, the dual partition of $R$.  Furthermore, for each $j=1,2,\dots,r_1$,
we have the inequality $\sum^{1\le i\le j} c''_i\le
\sum^{1\le i\le j} c_i$, since the operation of shift ones to
the left can only increase the partial sums.   It follows that 
$R^d\ge C$ in the poset $\cgP(t)$.

So here is the Gale-Ryser theorem.

\begin{theorem}
Let $R$ and $C$ be partitions of a positive integer $t$.  Then there
exists a zero--one matrix with row sum string $R$ and column sum
string~$C$ if and only if $R^d\ge C$ in the poset $\cgP(t)$.
\end{theorem}
\begin{proof}
The necessity of the condition has been established.  We prove sufficiency.
The proof is constructive.  In the poset $\cgP(t$, let
$W_0>W_1>\dots>W_s$ be a chain so that (1)~$W_0=R^d$, (2)~$W_s=C$ and
(3)~if $0\le p<s$, then $W_p$ covers $W_{p+1}$.   We start with a zero
one matrix $M_0$ having row sum string $R$ and column sum string $W_0$,
as suggested in \autoref{fig:dualpartition} for the partition
$(8,4,3,1,1,1)$.   If $s=0$, we are done, so we assume that for some $p$ with
$0\le p<s$, we have a zero--one matrix $M_p$ with row sum string $R$ and
column sum string $W_p$.  Then let $i$ and $j$ be the integers from
Proposition~\ref{prop:partitioncover}, which detail how $W_p$ covers
$W_{p+1}$.  Choose a row $q$ so that the $q,i$ entry of $M_p$ is $1$
while the $q,j$ entry of $M$ is $0$.  Exchange these two entries
to form the matrix $M_{p+1}$.  Note that the exchange may in fact
require adding a new column to the matrix.
\end{proof}


\section{Arithmetic Combinatorics}

In recent years, a great deal of attention has been focused on
topics in arithmetic combinatorics, with a number of deep and
exciting discoveries in the offing.  In some sense, this area
is closely aligned with ramsey theory and number theory, but recent work 
shows connections with real and complex analysis, as well.  Furthermore,
the roots of arithmetic combinatorics go back many years.  In this section, 
we present a brief overview of this rich and rapidly changing area.

Recall that an increasing sequence $a_1<a_2<a_3<\dots<a_t)$ of integers is
called an \textit{arithmetic progression} when there exists a
positive integer $d$ for which $a_{i+1}-a_i=d$, for all $i=1,2,\dots,t-1$.
The integer $t$ is called the \textit{length} of the arithmetic progression.

\begin{theorem}
For pair $r,t$ of positive integers, there exists an integer $n_0$, so that
if $n\ge n_0$ and $\phi:\{1,2,\dots,n\}\rightarrow\{1,2,\dots,r\}$ is any
function, then there exists a $t$-term arithmetic progression
$1\le a_1<a_2<\dots<a_t\le n$ and an element $\alpha\in\{1,2,\dots,r\}$
so that $\phi(a_i)=\alpha$, for each $i=1,2,\dots,t$.
\end{theorem}

Material will be added here.

\section{The Lovasz Local Lemma}

Even though humans seem to have great difficulty in 
providing expicit constructions for expontially large graphs 
which do not have complete subgraphs or independent sets 
of size~$n$, such graphs exist with great abundance.  Just 
take one at random and you are almost certain to get one.  And as a general
rule, probabilistic techniques often provide a method for
finding something that readily exists, but is hard to find.

Similarly, in the probabilistic proof that there exist
graphs with large girth and large chromatic number, we actually
showed that almost all graphs have modest sized independence
number and relatively few small cycles, provided that the
edge probability is chosen appropriately. The small cycles can
be destroyed without significantly changing the size of the
graph.

By way of contrast, probabilistic techniques can, in certain
circumstances, be used to find something which is exceedingly
rare.  We next present an elegant but elementary result, known
as the Lov\'asz Local Lemma, which has
proved to be very, very powerful.  The treatment is simplified
by the following natural notation.   When $E$ is an event
in a probability space, we let $\overline{E}$ denote the complement
of $E$.  Also, when $\cgF=\{E_1,E_2,\dots,E_k\}$ we
let
\[
\prod_{E\in\cgF}E=\prod_{i=1}^k E_i=E_1E_2E_3\dots E_k
\]
denote the event $E_1\cap E_2\cap\dots\cap E_k$, i.e.,
concatenation is short hand for intersection.  These notations
can be mixed, so $E_1\overline{E_2}\overline{E_3}$ represents
$E_1\cap \overline{E_2}\cap\overline{E_3}$.   Now let $\cgF$
be a finite family of events, let $E\in\cgF$ and let $\cgN$
be a subfamily of $\cgF-\{E\}$.  In the statement of the lemma 
below, we will say that $E$ is independent of any event not in 
$\cgN$ when
\[
P(E|\prod_{F\in\cgG}\overline{F}=P(E)
\]
provided $\cgG\cap\cgN=\emptyset$.

We first state and prove the lemma in \textit{asymmetric} form.
Later, we will give a simpler version which is called the
\textit{symmetric} version.

\begin{lemma}[Lov\'asz Local Lemma]\label{lem:LLL}
Let $\cgF$ be a finite family of events in a probability space 
and for each event $E\in\cgF$, let $\cgN(E)$ denote a
subfamily of events from $\cgF-\{E\}$ so that $E$ is independent of 
any event not in $\cgN(E)$.  Suppose that
for each event $E\in\cgF$, there is a real number $x(E)$ with
$0<x(E)<1$ such that
\[
P(E)\le x(E)\prod_{F\in\cgN(E)}(1-x(F)).
\]
Then for every non-empty subfamily $\cgG\subseteq\cgF$,
\[
P(\prod_{E\in\cgG}\overline{E})\ge\prod_{E\in\cgG}(1-x(E)).
\]
In particular, the probability that all events in $\cgF$ 
fail is positive.
\end{lemma}
\begin{proof}
We proceed by induction on $\cgG$. If $|\cgG|=1$ and $\cgG=\{E\}$,
we are simply asserting that $P(\overline{E})\ge 1-x(E)$, which is true
since $P(E)\le x(E)$.  Now suppose that $|\cgG|=k\ge2$ and that the lemma
holds whenever $1\le |\cgG|<k$.  Let $\cgG =\{E_1,E_2,\dots,E_k\}$.
Then
\[
P(\prod_{i\ge1}^k \overline{E_i})=P(\overline{E_1}|\prod_{i=2}^k\overline{E_i})
  P(\overline{E_2}|\prod_{i=3}^k\overline{E_i})P(E_3|\prod_{i=4}^k\overline{E_i})\dots
\]
Now each term in the product on the right has the following
form:
\[
P(\overline{E}|\prod_{F\in\cgF_E}\overline{F})
\]
where $|\cgF_E|<k$.

So, we done if we can show that
\[
P(\overline{E}|\prod_{F\in\cgF_E}\overline{F})\ge 1- x(E)
\]

This is equivalent to showing that
\[
P(E|\prod_{F\in\cgF_E}\overline{F})\le x(E)
\]

Suppose first that $\cgF_E\cap\cgN(E)=\emptyset$.  Then
\[
P(E|\prod_{F\in\cgF_E}\overline{F})=P(E)\le x(E).
\]
So we may assume that $\cgF_E\cap\cgN(E)\neq\emptyset$.  Let
$\cgF_E=\{F_1,F_2,F_r,F_{r+1},F_{r+2},\dots, F_t\}$, with
$F_i\in\cgN_E$ if and only if $r+1\le i\le t$.  Then

\[
PE|\prod_{F\in\cgF_E}\overline{F})=
 \frac{P(E\prod_{F\in\cgF_E\cap\cgN(E)}\overline{F}|\prod_{F\in\cgF_E-\cgN(E)}\overline{F})}
 {P(\prod_{F\in\cgF_E\cap \cgN(E)}\overline{F})}
\]

Consider first the numerator in this last expression.  Note that
\[
P(E\prod_{F\in\cgF_E\cap\cgN(E)}\overline{F}|\prod_{F\in\cgF_E-\cgN(E)}\overline{F})\le
P(E|\prod_{F\in\cgF_E\cap\cgN(E)}\overline{F})\le x(E)\prod_{F\in\cgF_E\cap\cgN(E)}(1-x(F))
\]
Next, consider the denominator.  By the inductive hypothesis, we have
\[
P(\prod_F\in\cgF_E\cap\cgN(E)\overline{F}\ge \prod_{F\in\cgF_E\cap\cgN(E)}(1-x(F)).
\] 
Combining these last two inequalities, we have
\[
P(E|\prod_{F\in\cgF_E}\overline{F})\le x(E)\prod_{\cgN(E)-\cgF_E}(1-x(F))\le x(E),
\]
and the proof is complete.
\end{proof}

Now here is the symmetric version.

\begin{lemma}[Lov\'asz Local Lemma]\label{lem:LLL-sym}
Let $p$ and $d$ be numbers with $0<p<1$ and $d\ge 1$.  Also,
let $\cgF$ be a finite family of events in a probability space 
and for each event $E\in\cgF$, let $\cgN(E)$ denote the 
subfamily of events from $\cgF-\{E\}$ so that $E$ is independent of 
any event not in $\cgN(E)$.  Suppose that $P(E)\le p$, $|\cgN(E)|\le d$ for 
every event $E\in\cgF$ and that $ep(d+1)<1$, where 
$e=2.71828\dots$ is the base for natural logarithms.  Then
Then
\[
P(\prod_{E\in\cgF}\overline{E})\ge\prod_{E\in\cgG}(1-x(E)),
\]
i.e., the probability that all events in $\cgF$ is positive.
\end{lemma}
\begin{proof}
Set $x(E)=1/(d+1)$ for every event $E\in\cgF$.  Then
\[
P(E)\le p\le \frac{1}{e(d+1)}\le x(E)\prod(F\in\cgN(E)(1-\frac{1}{d+1}).
\]
\end{proof}

A number of applications of the symmetric form of the Lov\'asz Local
Lemma are stated in terms of the condition that $4pd<1$.  The proof
of this alternate form is just a trivial modification of the argument
we have presented here.

\section{Applying the Local Lemma}

The list of applications of the Local Lemma has been growing
steadily, as has the interest in how the lemma can be applied
algorithmically, i.e., in a constructive setting.  But here
we present one of the early applications to Ramsey theory---estimating
the Ramsey number $(R,3,n)$.  Recall that we have the basic inequality 
$R(3,n)\le \binom{n+1}{3}$ from \autoref{thm:graphramsey}, and it is
natural to turn to the probabilistic method to look for good lower bounds.
But a few minutes thought shows that there are challenges to this
approach.

First, let's try a direct computation.  Suppose we try a random graph
on $t$ vertices with edge probability $p$.  So we would want no triangles,
and that would say we need $t^3p^3=1$, i.e., $p=1/t$.  Then we
would want no independent sets of size $n$, which would require
$n^te^{-pn^2}=1$, i.e., $t\ln n=pn^2$, so we can't even make
$t$ larger than $n$.  That's not helpful.

We can do a bit better by by allowing some triangles and then removing
one point from each, as was done in the proof for
\autoref{thm:girth}. Along these lines, we would
set $t^3p^3=t$, i.e., $p=t^{-2/3}$.  And the calculation now
yields the lower bound $R(3,n)\ge n^{6/5}/\ln^{-3/5} n$, so even
the exponent of $n$ is different from the upper bound.

So which one is right, or is the answer somewhere in between?
In a classic 1961 paper, Erd\H{o}s used a very clever application
of the probabilistic method to show the existence of a graph from
which a good lower bound could be extracted.  His technique
yielded the lower bound $R(3,n)\ge n^2/\ln^2 n$, so the two
on the exponent of $n$ is correct.

Here we will use the Lov\'asz Local Lemma to obtain this
same lower bound in a much more direct manner.  We consider
a random graph on $t$ vertices with edge probability $p$.
For each $3$-element subset $S$, we have the event $E_S$ which is
true when $S$ forms a triangle.  For each $n$-element set $T$, we have
the event $E_T$ which is true when $T$ is an independent set.  In
the discussion to follow, we abuse notation slightly and refer to
events $E_S$ and $E_T$ as just $S$ and $T$, respectively.  Note
that the probability of $S$ is $p^3$ for each $3$-element
set $S$, while the probability of $T$ is $q=(1-p)^{C(n,2)}\sim
e^{-pn^2/2}$ for each $n$-element set $T$.  

When we apply the Local Lemma, we will set $x=x(S)$ to be $e^2p^3$,
for each $3$-element set $S$.  And we will set $y=Y(T)=q^{1/2}\sim
e^{-pn^2/4}$.  It will be clear in a moment where we got those values.

Furthermore, the neighborhood of an event consists of all sets
in the family which have two or more elements in common.
So the neighborhood of a $3$-element set $S$ consists of $3(t-3)$ 
other $3$-element sets and $C(t-3,n-3)+3C(t-3,n-2)$ sets of size~$n$.  
Similarly, the neighborhood of an $n$-element set $T$ consists of $C(n,3)+
(t-n)C(n,2)$ sets of size $3$ and $\sum_{i=2}^{n-1}C(n,i)
C(t-n,n-i)$ other sets of size~$n$.  So the basic inequalities
we need to satisfy are:

\begin{align*}
p^3 \le & x(1-x)^{3(t-3)}(1-y)^{C(t-3,n-3)+3C(t-n,n-2)}\\
q \le & y(1-x)^{C(n,3)+(t-n)C(n,2)}(1-y)^{C(t-3,n-3)+3C(t-n,n-2)}\\
\end{align*}

Next, we assume that $n^{3/2}<t<n^2$ and then make the usual 
approximations, ignoring smaller order terms and multiplicative
constants, to see that these inequalities can be considered in 
the following simplified form:

\begin{align*}
p^3 \le & x(1-x)^{t}(1-y)^{t^n}\\
q \le & y(1-x)^{tn^2}(1-y)^{t^n}\\
\end{align*}

A moments reflection makes it clear that
we want to keep the terms involving
$(1-y)$ relatively large, i.e.,
at least $1/e$.  This will certainly be true if
we keep  $t^n\le 1/y$. This is equivalent
to $n\ln t\le pn^2$, or $\ln t\le pn$.

Similarly, we want to keep the term $(1-x)^{t}$ relatively large,
so we keep $t\le 1/x$, i.e., $t\le 1/p^3$.  On the other hand,
we want only to keep the term $(1-x)^{tn^2}\sim e^{-xtn^2}$ at least as large
as $y$.  This is equivalent to keeping
$p\le xt$, and since $x\sim p^3$, this can be rewritten as $p^{-1}\le t^{1/2}$.  

Now we have our marching orders.  We just set $\ln t=pn$ and
$p^{-1}=t^{1/2}$.  After substituting, we get $t= n^2/\ln^2t$ and
since $\ln t=\ln n$ (at least within the kind of approximations we
are using), we get the desired result $t=n^2/\ln^2n$.


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "book"
%%% End: 
