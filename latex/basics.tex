% basics.tex
% Updated January 11, 2012

\chapter{Combinatorial Basics}\label{ch:basics}

Dave hates doing the same thing twice.  He sees himself
as a free spirit and never wants to fall into a rut.  Alice
says that this approach to life requires one to have lots
and lots of options, for if you have to do a lot of something,
like get up in the morning and get dressed, then you may not
be able to avoid mindless repetition, dull and boring as it
may seem.

\section{The Pigeon Hole Principle}\label{s:basics:pigeonhole}

A function $f:X\longrightarrow Y$ is said to be $1$--$1$ when
$f(x)\neq f(x')$ for all $x,x'\in X$ with $x\neq x'$.  A $1$--$1$
function is also called an \textit{injection}.  When
$f:X\longrightarrow Y$ is $1$--$1$, we note that $|X|\le |Y|$.
Conversely, we have the following self-evident statement, which is
popularly called the ``Pigeon Hole'' principle.

\begin{proposition}\label{prop:pigeon}
If $f:X\longrightarrow Y$ is a function and $|X|>|Y|$, then
there exists an element $y\in Y$ and distinct elements $x,x'\in X$
so that $f(x)=f(x')=y$.
\end{proposition}

In more casual language, if you must put $n+1$ pigeons into $n$ holes,
then you must put two pigeons into the same hole.

Here is a classic result, whose proof follows immediately from
the Pigeon Hole principle.

\begin{theorem}[Erd\H{o}s/Szekeres]\label{thm:ErdosSzekeres}
If $m$ and $n$ are non-negative integers, then 
any sequence of $mn+1$ distinct real numbers 
either has an increasing subsequence of $m+1$ terms, or it has a 
decreasing subsequence of $n+1$ terms.
\end{theorem}
\begin{proof}
Let $\sigma=(x_1,x_2,x_3,\dots,x_{mn+1})$ be a sequence of
$mn+1$ distinct real numbers.  For each $i=1,2,\dots,mn+1$, let $a_i$ be the
maximum number of terms in a increasing subsequence of
$\sigma$ with $x_i$ the first term.  Also, let $b_i$ be the maximum number
of terms in a decreasing subsequence of $\sigma$ with $x_i$
the last term.  If there is some $i$ for which $a_i\ge m+1$, then
$\sigma$ has an increasing subsequence of $m+1$ terms.  Conversely, if
for some $i$, we have $b_i\ge n+1$, then we conclude that $\sigma$ has
a decreasing subsequence of $n+1$ terms.

It remains to consider the case where $a_i\le m$ and $b_i\le n$ for all
$i=1,2,\dots,mn+1$.  Since there are $mn$ ordered pairs of the form
$(a,b)$ where $1\le a\le m$ and $1\le b\le n$, we conclude from the
Pigeon Hole principle that there must be integers $i_1$ and $i_2$ with
$1\le i_1<i_2\le mn+1$ for which $(a_{i_1},b_{i_1})=(a_{i_2},b_{i_2})$.
Since $x_{i_1}$ and $x_{i_2}$ are distinct, we either have $x_{i_1}<x_{i_2}$
or $x_{i_1}>x_{i_2}$.  In the first case, any increasing subsequence of 
with $x_{i_2}$ as its first term can be extended by prepending
$x_{i_1}$ at the start.  This shows that $a_{i_1}>a_{i_2}$.  
In the second case, any decreasing sequence of with $x_{i_1}$ as its last 
element can be extended by adding $x_{i_2}$ at the very end.  This
shows $b_{i_2}>b_{i_1}$.
\end{proof}

In \autoref{ch:probmeth}, we will explore some powerful
generalizations of the Pigeon Hole principle.  All these results have
the flavor of the general assertion that total disarray is impossible.

\section{An Introduction to Complexity Theory}\label{s:basics:complexity}

\begin{discussion}
Bob says that he's really getting to like this combinatorial
mathematics stuff. The concrete nature of the subject is appealing.
But he's not sure that he understands the algorithmic component.
Sometimes he sees how one might actually compute the answer to a
problem---provided he had access to a powerful computer.  At other
times, it seems that a computational approach might be out of
reach, even with the world's best and fastest computers at ready
access.  Carlos says it can be much worse than that.  There are
easily stateable problems that no one knows how to attack even
if all the world's computational power is used in concert.  And there's nothing on the
horizon that will change that.  In fact, build faster computers and
you just change the threshold for what is computable.  There will still
be easily understood problems that will remain unresolved.
\end{discussion}

\subsection{Three Questions}\label{s:basics:questions}

We consider three problems with a common starting point.  You are
given\footnote{The particulars of how the set is given to you
aren't important to the discussion.  For example, the data could be given
as a text file, with one number on each line.} a set $S$ of 
$10,000$ distinct positive integers, each at most
$100,000$,  and then asked the following questions.

\begin{enumerate}
\item Is $83,172$ one of the integers in the set $S$?
\item Are there three integers in $S$ whose sum is $143,297$?
\item Can the set $S$ be partitioned as $S=A\cup B$ with
$A\cap B=\emptyset$, so that $\sum_{a\in A}a=\sum_{b\in B}b$.
\end{enumerate}

The first of the three problems sounds easy, and it is.  You just consider
the numbers in the set one by one and test to see if any of them is
$83,172$.  You can stop if you ever find this number and
report that the answer is yes.  If you return a no answer, then you
will have to have read every number in the list.  Either way, you halt
with a correct answer to the question having done at most $10,000$
tests, and even the most modest netbook can do this in a heartbeat.
And if the list is expanded to $1,000,000$ integers, all at most
a billion, you can still do it easily.  More generally, if you're
given a set $S$ of $n$ numbers and an integer $x$ with the question
``Is $x$ a member of $S$?'', you can answer this question in
$n$ steps, with each step an operation of testing a number in $S$
to see if it is exactly equal to $n$.  So the running time of this
algorithm is proportional to $n$, with the constant depending on
the amount of time it takes a computer to perform the basic operation
of asking whether a particular integer is equal to the target value.

The second of the three problems is a bit more challenging.  Now it seems
that we must consider the $3$-element subsets of a set of size $10,000$.
There are $C(10,000,3)$ such sets.  On the one hand, testing three
numbers to see if their sum is $143,297$ is very easy, but there are lots
and lots of sets to test.  Note that $C(10,000,3)=166,616,670,000$, and not
too many computers will handle this many operations.  Moreover, if the
list is expanded to a million numbers, then we have more than $10^{17}$
triples to test, and that's off the table with today's hardware.

Nevertheless, we can consider the general case.  We are given a set $S$ of $n$
integers and a number $x$.  Then we are asked whether there are
three integers in $S$ whose sum is $x$.  The algorithm we have
described would have running time proportional to $n^3$, where the constant
of proportionality depends on the time it takes to test a triple of
numbers to see if there sum is $x$.  Of course, this depends in turn
on just how large the integer $x$ and the integers in $S$ can be.

The third of the three problems is different. First, it seems to be much harder.
There are $2^{n-1}$ complementary pairs of subsets of a set of size~$n$,
and one of these involves the emptyset and the entire set.  But that
leaves $2^{n-1}-1$ pairs to test.  Each of these tests is not all that
tough.  A netbook can easily report whether a two subsets have the
same sum, even when the two sets form a partition of a set of size
$10,000$, but there are approximately $10^{3000}$ partitions to test
and no piece of hardware on the planet will touch that assignment.
And if we go up to a set of size $1,000,000$, then the combined computing
power of all the machines on earth won't get the job done.

In this setting, we have an algorithm, namely testing all partitions,
but it is totally unworkable for $n$ element sets when $n$ is large since 
it has running time proportional to $2^n$.

\subsection{Certificates}

Each of the three problems we have posed is in the form of a
``yes/no'' question. A ``yes'' answer to any of the three can be justified
by providing a certificate.  For example, if you answer the first
question with a yes, then you might provide the additional information
that you will find $83,172$ as the integer on line $584$ in the input
file.  Of course, you could also provide the source code for the
computer program, and let a referee run the entire procedure.

Similarly, if you answer the second question with a yes,
then you could specify the three numbers and specify where in
the input file they are located.  An impartial referee could then
verify, if it mattered, that the sum of the three integers was
really $143,297$ and that they were located at the specified places
in the input file.  Alternatively, you could again provide the source
code which would require the referee to test all triples and
verify that there is one that works.

Likewise, a yes for the third question admits a modest size
certificate.  You need only specify the elements of the subset $A$.
The referee, who is equipped with a computer, can (a)~check to see that
all numbers in $A$ belong to $S$; (b)~form a list of the subset $B$ consisting
of those integers in $S$ that do not belong to $A$; and (c)~compute
the sums of the integers in $A$ and the integers in $B$ and verify that
the two sums are equal.  But in this case, you would not provide 
source code for the algorithm, as there does not appear (at least nothing
in our discussion thus far provides one) to be a reasonable strategy
for deciding this problem when the problem size is large.

Now let's consider the situation with a ``no'' answer.  When the answer
to the first question is no, the certificate can again be a computer program
that will enable the referee to consider all the elements of $S$ and
be satisfied that the number in question is not present.  A similar
remark holds for the second question, i.e., the program is the certificate.

But the situation with the third question is again very different.
Now we can't say to the referee ``We checked all the possibilities and
none of them worked.''  This could not possibly be a true statement.
And we have no computer program that can be run by us or by the 
referee.  The best we could say is that we tried to find a suitable partition 
and were unable to do so. As a result, we don't know what the correct answer 
to the question actually is.

\subsection{Operations}

Many of the algorithms we develop in this book, as well as many of
the computer programs that result from these algorithms involve
basic steps that are called \textit{operations}.  The meaning of
the word operation is intentionally left as an imprecise notion.
An operation might be just comparing two integers to see if they
are equal; it might be updating the value of a variable $x$ and
replacing it by $x^2-3x+7$; and it might be checking whether two
set sums are equal.  In the third instance, we would typically
limit the size of the two subsets as well as the integers in them.
As a comsequence, we want to be able to say that there is some
constant $c$ so that an operation can be carried out in time
at most $c$ on a computer. Different computers yield different
values of $c$, but that is a discrepancy which we can safely ignore.

\subsection{Input Size}

Problems come in various sizes.  The three problems we have discussed
in this chapter have the same input size.   Roughly speaking this
size is $10,000$ blocks, with each block able to hold an integer
of size at most $100,000$.  In this text, we will say that the
input size of this problem is $n=10,000$, and in some sense ignoring
the question of the size of the integers in the set.  There are 
obvious limitations to this approach.  We could be given a set $S$
of size~$1$ and a candidate element $x$ and be asked whether $x$ belongs
to $S$.  Now suppose that $x$ is a bit string the size of a typical
compact disk, i.e., some $700$ megabytes in length.  Just reading the single
entry in $S$ to see if it's exactly $x$ will take some time.

In a similar vein, consider the problem of determing whether a file
$x$ is located anywhere in the directory structure under $y$ in
a unix file system.  If you go on the basis of name only, then
this may be relatively easy.  But what if you want to be sure that an exact copy
of $x$ is present.  Now it is much more challenging.

\section{The Big ``Oh'' and Little ``Oh'' Notations}\label{s:basics:big-oh}

Let $f:\mathbb{N}\longrightarrow \mathbb{R}$ and
$g:\mathbb{N}\longrightarrow\mathbb{R}$ be functions.
We write $f=O(g)$, and say $f$ is ``Big Oh'' of $g$,  
when there is a constant $c$ and an integer
$n_0$ so that $f(n)\le cg(n)$ whenever $n>n_0$.
Although this notation has a long history, we can provide
a quite modern justification.  If $f$ and $g$ both describe
the number of operations  required for two algorithms given input
size $n$, then the meaning of $f=O(g)$ is that $f$ is no
harder than $g$ when the problem size is large.

We are particulary interested in comparing functions against
certain natural benchmarks, e.g.,  $\log\log n$, $\log n$, $\sqrt{n}$, 
$n^\alpha$ where $\alpha<1$,  $n$, $n^2$, $n^3$, $n^c$ where $c>1$ 
is a constant, $n^{\log n}$, $2^n$, $n!$, $2^{n^2}$, etc.

For example, later in this text, we will learn that there
are sorting algorithms with running time $O(n\log n)$ where
$n$ is the number of integers to be sorted.  As a second 
example, we will learn that we can find all shortest paths in 
an oriented graph on $n$ vertices with non-negative weights on edges with an 
algorithm having running time $O(n^2)$.  At the other extreme,
no one knows whether there is a constant $c$ and an algorithm for 
determining whether the chromatic number of a graph is at most three
which has running time $O(n^c)$.  

It is important to remember that when we write $f=O(g)$, we are
implying in some sense that $f$ is no bigger than $g$, but it
may in fact be much smaller.  By contrast, there will be times
when we really know that one function dominates
another.  And we have a second kind of notation to capture this
relationship.

Let $f:\mathbb{N}\longrightarrow \mathbb{R}$ and
$g:\mathbb{N}\longrightarrow\mathbb{R}$ be functions with
$f(n)>0$ and $g(n)>0$ for all $n$.  We write $f=o(g)$, and 
say that $f$ is ``Little oh'' of $g$, when 
$\lim_{n\rightarrow\infty}f(n)/g(n)=0$. 
For example $\ln n=o(n^{.2})$; $n^\alpha=o(n^{\beta})$ whenever
$0<\alpha<\beta$; and $n^{100}=o(c^n)$ for every $c>1$.
In particular, we write $f(n)=o(1)$ when $\lim_{n\rightarrow\infty}f(n)=0$.

\section{Exact Versus Approximate}\label{s:basics:exact}

Many combinatorial problems admit ``exact'' solutions, and in these
cases, we will usually try hard to find them.  The Erd\H{o}s/Szekeres
theorem from earlier in this chapter is a good example of an
``exact'' result\footnote{Exact results are also called ``best possible'', ``sharp'' 
or ``tight.''}.  By this statement,
we mean that for each pair $m$ and $n$ of positive integers, there
is a sequence of $mn$ distinct real numbers that has neither
an increasing subsequence of size $m+1$ nor a decreasing subsequence
of size~$n+1$.  To see this, consider the sequence $\sigma$ defined
as follows: For each $i=1,2,\dots,m$,
let $B_i=\{j+(m-1)i:1\le j\le n\}$.  Note that each $B_i$ is a block
of $n$ consecutive integers.  Then define a permutation $\sigma$ of
the first $mn$ integers by setting $\alpha<\beta$ if there exist
distinct integers $i_1$ and $i_2$ so that $\alpha\in B_{i_1}$ and
$\beta\in B_{i_2}$.  Also, for each $i=1,2,\dots,m$, set $\alpha<\beta$
in $\sigma$ when $1+(m-1)i\le \beta<\alpha\le n+(m-1)i$.  Clearly,
any increasing subsequence of $\sigma$ contains at most one member
from each block, so $\sigma$ has no increasing sequence of size~$m=1$.
On the other hand, any decreasing sequence in $\sigma$ is contained
in a single block, so $\sigma$ has no decreasing sequence of size $n+1$.

As another example of an exact solution, the number of integer solutions 
to $x_1+x_2+\dots x_r=n$ with $x_i>0$ for $i-1,2,\dots,r$ is exactly $C(n-1,r-1)$.
On the other hand, nothing we have discussed thus far allows us to provide
an exact solution for the number of partitions of an integer~$n$.

\subsection{Approximate and Assymptotic Solutions}

Here's an example of a famous problem that we can only discuss in terms of
approximate solutions, at least when the input size is suitably large.
For an integer $n$, let $\pi(n)$ denote the number of primes among the 
first $n$ positive integers.  For example, 
$\pi(12)=5$ since $2$, $3$, $5$, $7$ and $11$ are primes.   
The exact value of $\pi(n)$ is known when $n\le 10^{23}$, and in fact: 
\[
\pi(10^{23}) = 1,925,320,391,606,803,968,923
\]
On the other hand, you might ask whether $\pi(n)$ tends to infinity
as $n$ grows larger and larger.  The answer is yes, 
and here's a simple and quite classic argument.  Suppose
to the contrary that there were only $k$ primes, where $k$ is a positive
integer.  Suppose these $k$ primes are listed in increasing order as
$p_1<p_2<\dots<p_k$, and consider the number $n=1+p_1p_2\cdots p_k$.
Then $n$ is not divisible by any of these primes, and it is larger than
$p_k$, which implies that $n$ is a prime number larger than $p_k$.

So we know that $\lim_{n\rightarrow\infty}\pi(n)=\infty$.
In a situation like this, mathematicians typically want to know more about 
how fast $\pi(n)$ goes to infinity.  Some functions go to infinity ``slowly'',
such as $\log n$ or $\log\log n$.  Some go to infinity quickly, like
$2^n$, $n!$ or $2^{2^n}$.  Since $\pi(n)\le n$, it can't go to infinity
as fast as these last three functions, but it might go infinity like
$\log n$ or maybe $\sqrt{n}$. 

On the basis of computational results (done by hand, long before there
were computers), Legendre conjectured in 1796 that $\pi(n)$ goes to
infinity like $n/\ln n$.  To be  more precise, he conjectured that
\[
\lim_{n\rightarrow\infty}\frac{\pi(n)\ln n}{n}=1.
\]
In 1896, exactly one hundred years after Legendre's conjecture, 
Hadamard and de la Vall\'ee-Poussin independently published proofs
of the conjecture, using techniques whose roots are in the
Riemann's pioneering work in complex analysis.  This result, now
known simply as the \textit{Prime Number Theorem}, continues to this
day to be much studied topic at the boundary of analysis and number
theory.

\subsection{Polynomial Time Algorithms}

Throughout this text, we will place considerable emphasis on
problems which admit polynomial time solutions.  This refers
to problems for which there is some constant $c>0$ so that there
is an algorithm $\cgA$ for solving the problem which has running
time $O(n^c)$ where $n$ is the input size.
the symbol $\cgP$ is suggestive of \textit{polynomial}.


\subsection{$\mathcal{P}=\mathcal{NP}$?}

Perhaps the most famous question at the boundary of
combinatorial mathematics, theoretical computer science
and mathematical logic is the notoriously challenging
question of deciding whether $\mathcal{P}$ is the same
as $\mathcal{NP}$.  This problem has the shorthand form:
$\mathcal{P}=\mathcal{NP}$?  Here, we present
a brief informal discussion of this problem.

First, we have already introduced the class $\cgP$ consisting of all 
yes-no combinatorial problems which admit polynomial time algorithms.
The first two problems discussed in this chapter belong to
$\cgP$ since they can be solved with algorithms that
have running time $O(n)$ and $O(n^3)$, respectively.
Also, determing whether a graph is $2$-colorable and whether it
is connected both admit polynomial time algorithms.  

We should emphasize that it may be very difficult to determine
whether a problem belongs to class $\mathcal{P}$ or not.
For example, we don't see how to give a fast algorithm for
solving the third problem (subset sum), but that doesn't mean
that there isn't one.  Maybe we all need to study harder!

Setting that issue aside for the moment, the class $\mathcal{NP}$ consists of
yes--no problems for which there is a certificate for
a yes answer whose correctness can be verified in polynomial
time. More formally, this is called the class of \textit{nondeterministic
polynomial time} problems.  Our third problem definitely belongs to this class.

So the famous question is to detemine whether the two
classes are the same.  Evidently, any problem belonging
to $\cgP$ also belongs to $\mathcal{NP}$, i.e,
$\cgP\subseteq\mathcal{NP}$, but are they equal?
It seems difficult to believe that there is a polynomial
time algorithm for settling the third problem (the subset
sum problem), and no one has come close to settling this
issue.  But if you get a good idea, be sure to discuss it with
one or both authors of this text before you go public with
your news.  If it turns out that you are right, you are
certain to treasure a photo opportunity with yours truly.

\section{Discussion}\label{s:basics:discussion}

Carlos, Dave and Yolanda were fascinated by the discussion on complexity.
Zori was less enthusiastic but even 
she sensed that the question of which problems could be solved quickly
had practical implications. She could even predict that people could
earn a nice income solving problems faster and more accurately
than their competition.

Bob remarked ``I'm not sure I understand what's being talked about
here.  I don't see why it can't be the  case that all problems
can be solved.  Maybe we just don't know how to do it.''
Xing said ``Any finite problem \textit{can} be solved.  There
is always a way to list all the possibilities, compare them
one by one and take the best one as the answer.''
Alice joined in ``Well, a problem might take a long time
just because it is big.  For example, suppose you are given
two dvd's, each completely full with the data for a large integer.
How are you possibly going to multiply them together, even
with a large computer and fancy software.''  Carlos then
offered ``But I think there are really hard problems that
any algorithm will take a long time to solve and not just
because the input size is large.  At this point, I don't know how 
to formulate such a problem but I suspect that they exist.''

\section{Exercises}\label{s:basics:exercises}

\begin{enumerate}
\item  Suppose you are given a list of $n$ integers, each of size
at most $100n$.   How many operations would it take you to do the following
tasks (in answering these questions, we are interested primarily in whether it
will take $\log n$, $\sqrt{n}$, $n$, $n^2$, $n^3$, $2^n$, \dots steps.  In 
other words, ignore multiplicative constants.):

\begin{enumerate} 
\item Determine if the number $2n+7$ is in the list.
\item Determine if there are two numbers in the list whose sum is
$2n+7$.
\item Determine if there are two numbers in the list whose product is
$2n+7$ (This one is more subtle than it might appear!  It may be
to your advantage to sort the integers in the list).
\item Determine if there is a number $i$ for which all the numbers in
the list are between $i$ and $i+2n+7$. 
\item Determine  the longest sequence of consecutive integers belonging
to the list.
\item Determine the number of primes in the list. 
\item Determine whether there are three integers $x$, $y$ and $z$ from
the list so that $x+y=z$.
\item Determine whether there are three integers $x$, $y$ and $z$ from
the list so that $x^2+y^2=z^2$.
\item Determine whether there are three integers $x$, $y$ and $z$ from
the list so that $xy=z$.
\item Determine whether there are three integers $x$, $y$ and $z$ from
the list so that $x^y=z$.
\item Determine whether there are two integers $x$ and $y$ from
the list so that $x^y$ is a prime.
\item Determine the longest arithmetic progression in the list (a
sequence $(a_1, a_2,\dots,a_t)$ is an arithmetic progression when
there is a constant $d\neq 0$ so that $a_{i+1}=a_i+d$, for each $i=1,2,
\dots, t-1$).
\item Determine the number of distinct sums that can be formed from
members of the list (arbitrarily many integers from the list are allowed to
be terms in the sum).
\item Determine the number of distinct products that can be formed from
members of the list (arbitrarily many integers from the list are allowed to be
factors in the product).
\item Determine for which integers $m$, the list contains at least $10\%$ of the
integers from $\{1,2,\dots,m\}$.

\end{enumerate}

\item If you have to put $n+1$ pigeons into $n$ holes, you have to
put two pigeons into the same hole.  What happens if you have to
put $mn+1$ pigeons into $n$ holes?

\item Consider the set $X=\{1,2,3,4,5\}$ and suppose you have two holes.
Also suppose that you have $10$ pigeons: the $2$-element subsets of
$X$.  Can you put these $10$ pigeons into the two holes in a way that there
is no $3$-element subset $S=\{a,b,c\}\subset X$ for which all pigeons from
$S$ go in the same hole?  Then answer the same question if $X=\{1,2,3,4,5,6\}$
with $15 = C(6,2)$ pigeons.

\item  Let $n=10,000$. Suppose a friend tells you that he has a secret family of subsets
of $\{1,2,\dots,n\}$, and if you guess it correctly, he will give you one million
dollars.  You think you know the subset he has in mind and it contains exactly
half the subsets, i.e., the family has $2^{n-1}$ subsets.  Discuss how you can
share your hunch with your friend in an effort to win the prize.   

\item  Let $N$ denote the set of positive integers.  When $f:N\rightarrow N$ is
a function, let $E(f)$ be the function defined by $E(f)(n) = 2^{f(n)}$.
What is $E^5(n^2)$?

\end{enumerate}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "book"
%%% End: 
